---
phase: 01-specification
plan: 04
type: execute
---

<objective>
Document Steps 9-11 (Backtest Passes, Monte Carlo, Generate Reports) - the analysis phase.

Purpose: Create complete specifications for backtesting selected passes, risk simulation, and report generation.
Output: 3 spec files (steps 9, 10, 11) in `.planning/phases/01-specification/specs/`
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-specification/01-CONTEXT.md
@.planning/phases/01-specification/01-03-SUMMARY.md

**Source files to analyze:**
@engine/runner.py (steps 9, 10, 11 implementations)
@engine/gates.py (backtest and monte carlo gates)
@modules/backtest.py (backtest execution)
@modules/monte_carlo.py (risk simulation)
@modules/trade_extractor.py (trade extraction from reports)
@reports/workflow_dashboard.py (dashboard generation)
@reports/leaderboard.py (leaderboard generation)
@reports/boards.py (boards view)
@settings.py (thresholds, weights, paths)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Document Step 9 (Backtest Top Passes)</name>
  <files>.planning/phases/01-specification/specs/step-09-backtest-passes.md</files>
  <action>Document the batch backtesting step:

  **From runner.py:**
  - Read `_step_backtest_passes()` method
  - Document how selected passes are backtested
  - Document batch execution (multiple passes in sequence)
  - Document deterministic report naming (critical for avoiding collisions)

  **From modules/backtest.py:**
  - Document backtest parameters
  - Document report parsing

  **From modules/trade_extractor.py:**
  - Document trade list extraction
  - Document trade data structure

  **Gates (from gates.py):**
  - Document per-pass gate conditions
  - Document what metrics are checked (profit factor, drawdown, etc.)
  - Document thresholds from settings.py

  **Output:**
  - Document backtest results structure
  - Document how results feed into Monte Carlo and reports
  - Document "best pass" selection criteria</action>
  <verify>Spec includes: batch execution, report naming, gate conditions, trade extraction</verify>
  <done>Step 9 fully documented</done>
</task>

<task type="auto">
  <name>Task 2: Document Step 10 (Monte Carlo)</name>
  <files>.planning/phases/01-specification/specs/step-10-monte-carlo.md</files>
  <action>Document the Monte Carlo risk simulation:

  **From runner.py:**
  - Read `_step_monte_carlo()` method
  - Document which pass is simulated (best pass from Step 9)

  **From modules/monte_carlo.py:**
  - Document simulation methodology
  - Document number of iterations
  - Document what gets randomized (trade order, trade skipping, etc.)
  - Document output metrics (worst case drawdown, probability of ruin, etc.)

  **Gate (from gates.py):**
  - Read `check_monte_carlo()` gate
  - Document pass/fail thresholds
  - Document what causes Monte Carlo to fail

  **Settings:**
  - Document configurable parameters (iterations, confidence level, etc.)

  **Output:**
  - Document Monte Carlo results structure
  - Document how results appear in dashboard</action>
  <verify>Spec includes: simulation methodology, output metrics, gate conditions</verify>
  <done>Step 10 fully documented</done>
</task>

<task type="auto">
  <name>Task 3: Document Step 11 (Generate Reports)</name>
  <files>.planning/phases/01-specification/specs/step-11-generate-reports.md</files>
  <action>Document the report generation step:

  **From runner.py:**
  - Read `_step_generate_reports()` method
  - Document that this step ALWAYS runs (even on earlier failures)
  - Document workflow status handling

  **From reports/workflow_dashboard.py:**
  - Document dashboard structure
  - Document what data gets embedded
  - Document navigation buttons (Boards, Leaderboard)

  **From reports/leaderboard.py:**
  - Document leaderboard aggregation
  - Document composite score calculation
  - Document sorting and ranking

  **From reports/boards.py:**
  - Document boards view
  - Document fallback behavior (Step 5 metrics when workflow fails later)

  **Output paths:**
  - Dashboard: `runs/dashboards/<workflow_id>/index.html`
  - Leaderboard: `runs/leaderboard/index.html`
  - Boards: `runs/boards/index.html`

  **Post-step refresh:**
  - Document that dashboards must be refreshed after Steps 12, 13 to show stress/forward results</action>
  <verify>Spec includes: all three report types, output paths, refresh requirement</verify>
  <done>Step 11 fully documented</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All 3 spec files exist
- [ ] Batch backtesting documented with deterministic naming
- [ ] Monte Carlo methodology and gates documented
- [ ] All three report types documented (dashboard, leaderboard, boards)
- [ ] Report refresh requirement documented
</verification>

<success_criteria>

- Steps 9, 10, 11 fully documented
- Backtest gate conditions specified with thresholds
- Monte Carlo simulation explained clearly
- Report generation flow complete
- Post-step refresh requirement captured
  </success_criteria>

<output>
After completion, create `.planning/phases/01-specification/01-04-SUMMARY.md`:

---
phase: 01-specification
plan: 04
subsystem: specification
requires: [step-8b-spec]
provides: [step-9-spec, step-10-spec, step-11-spec]
affects: [01-05, 02-core-domain]
key-decisions: []
key-files: [.planning/phases/01-specification/specs/step-09-backtest-passes.md, .planning/phases/01-specification/specs/step-11-generate-reports.md]
---

# Phase 01-specification Plan 04: Analysis Steps Summary

**[One-liner summary]**

## Accomplishments

- [Key outcomes]

## Files Created/Modified

- [List of spec files]

## Decisions Made

[Or "None"]

## Issues Encountered

[Or "None"]

## Next Step

Ready for 01-05-PLAN.md (Steps 12-14 post-analysis)
</output>
